{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a4c0a01",
   "metadata": {},
   "source": [
    "**Task:** Clustering on UCI seed dataset, which can be downloaded from https://archive.ics.uci.edu/ml/datasets/seeds. The number of clusters is set as 3.\n",
    "\n",
    "1. Implement K-means and GMM-EM algorithms from scratch (i.e., no third-party or off-the-shelf package or library are allowed). Explain briefly your source codes in the report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf8795c",
   "metadata": {},
   "source": [
    "**K-means** algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "f6cf7f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Picking up the lowest obtained objective value from 30 trial(s), we have the cluster sizes as [57, 65, 88] with 4 iterations\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "f = open(\"seeds_dataset.txt\", \"r\")\n",
    "data = []\n",
    "\n",
    "# data shape should be (210, 8) after read\n",
    "for line in f:\n",
    "    data.append([float(number) for number in line.split('\\t') if number != ''])\n",
    "\n",
    "data = [sample[:-1] for sample in data]\n",
    "    \n",
    "def cluster(data, K, is_random = False):\n",
    "    n = len(data)\n",
    "    d = len(data[0])\n",
    "    feature_range = [(min([data[i][feature] for i in range(n)]),\n",
    "                      max([data[i][feature] for i in range(n)])) for feature in range(d)]\n",
    "    max_distance = sum([(feature_range[feature][1] - feature_range[feature][0]) ** 2 for feature in range(d)])\n",
    "\n",
    "    c = [] # cluster centers\n",
    "    r = [-1 for i in range(n)] # assignments\n",
    "    \n",
    "    # Initialization: randomize K cluster centers c\n",
    "    c = [data[50 * k - 25] for k in range(K)]\n",
    "    if is_random == True:\n",
    "        c = random.sample(data, K)\n",
    "\n",
    "    number_of_iterations = 0\n",
    "    converged = False\n",
    "    while not converged:\n",
    "        \n",
    "        # Assignment\n",
    "        for i in range(n):\n",
    "            distance = max_distance\n",
    "            for k in range(K):\n",
    "                current_distance = sum([(data[i][feature] - c[k][feature]) ** 2 for feature in range(d)])\n",
    "                if current_distance < distance:\n",
    "                    distance = current_distance\n",
    "                    r[i] = k\n",
    "                    \n",
    "        # Refitting\n",
    "        clusters = [[data[i] for i in range(n) if r[i] == k] for k in range(K)]\n",
    "        \n",
    "        new_c = [[sum([clusters[k][i][feature] for i in range(len(clusters))]) / len(clusters) for feature in range(d)] if len(clusters[k]) > 0 else c[k] for k in range(K)]\n",
    "\n",
    "        if sum([abs(math.sqrt(sum([c[k][feature] ** 2 for feature in range(d)])) - math.sqrt(sum([new_c[k][feature] ** 2 for feature in range(d)]))) for k in range(K) if len(clusters[k]) > 0]) < 10 ** -6:\n",
    "            converged = True\n",
    "\n",
    "        c = [new_c[k] for k in range(K)]\n",
    "        number_of_iterations += 1\n",
    "\n",
    "    objective_value = sum([sum([(data[i][feature] - c[r[i]][feature]) ** 2 for feature in range(d)]) for i in range(n)])\n",
    "        \n",
    "    return clusters, objective_value, c, r, number_of_iterations\n",
    "        \n",
    "trials = 30\n",
    "ans = cluster(data, 3)\n",
    "for trial in range(trials - 1):\n",
    "    clusters, objective_value, c, r, number_of_iterations = cluster(data, 3)\n",
    "    if objective_value < ans[1]:\n",
    "        ans = [clusters, objective_value, c, r, number_of_iterations]\n",
    "\n",
    "print(\"Picking up the lowest obtained objective value from\", trials, \"trial(s), we have the cluster sizes as\", [len(ans[0][k]) for k in range(len(ans[0]))], \"with\", number_of_iterations, \"iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5dad67",
   "metadata": {},
   "source": [
    "**GMM-EM** algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "829e80e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current number of iterations: 5 (max number of iterations is set to 30)\n",
      "Current number of iterations: 10 (max number of iterations is set to 30)\n",
      "Current number of iterations: 15 (max number of iterations is set to 30)\n",
      "Current number of iterations: 20 (max number of iterations is set to 30)\n",
      "Current number of iterations: 25 (max number of iterations is set to 30)\n",
      "Current number of iterations: 30 (max number of iterations is set to 30)\n",
      "From a total of 30 iterations, we have the approximation of labeled data sizes as [67, 60, 83]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy.stats\n",
    "\n",
    "f = open(\"seeds_dataset.txt\", \"r\")\n",
    "data = []\n",
    "\n",
    "# data shape should be (210, 8) after read\n",
    "for line in f:\n",
    "    data.append([float(number) for number in line.split('\\t') if number != ''])\n",
    "\n",
    "data = [sample[:-1] for sample in data]\n",
    "    \n",
    "def GMM(data, K, is_random = False):\n",
    "    n = len(data)\n",
    "    d = len(data[0])\n",
    "    feature_range = [(min([data[i][feature] for i in range(n)]),\n",
    "                      max([data[i][feature] for i in range(n)])) for feature in range(d)]\n",
    "    \n",
    "    r = []\n",
    "    \n",
    "    mu = np.array([data[k] for k in range(K)]) # mean\n",
    "    if is_random == True:\n",
    "        mu = np.array(random.sample(data, K)) # mean\n",
    "    cov = np.array([np.cov(np.transpose(data)) for k in range(K)]) # covariacne\n",
    "    pi = np.array([1 / K for k in range(K)]) # mixing coefficients\n",
    "    \n",
    "    responsibilities = np.array([])\n",
    "    \n",
    "    def multivariate(x, mu, cov, custom = False):\n",
    "        if custom and np.linalg.det(cov) != 0:\n",
    "            pdf = 1\n",
    "\n",
    "            pdf *= 1 / ((2 * math.pi) ** (d / 2))\n",
    "            pdf *= np.linalg.det(cov) ** (-1/2)\n",
    "            pdf *= math.e ** (-1/2 * np.matmul(np.matmul(np.transpose(x - mu), np.linalg.inv(cov)), (x - mu)))\n",
    "        \n",
    "            return pdf\n",
    "        else:\n",
    "            return scipy.stats.multivariate_normal(mu, cov, allow_singular = True).pdf(x)\n",
    "    \n",
    "    number_of_iterations = 0\n",
    "    converged = False\n",
    "    log_likelihood = None\n",
    "\n",
    "    while not converged and number_of_iterations < 30:\n",
    "        \n",
    "        # E-step: Evaluate the responsibilities given current parameters\n",
    "        responsibilities = [[] for i in range(n)]\n",
    "        N = [0 for k in range(K)]\n",
    "        for i in range(n):\n",
    "            total = 0\n",
    "            for k in range(K):\n",
    "                responsibilities[i].append(pi[k] * multivariate(data[i], mu[k], cov[k]))\n",
    "                total += responsibilities[i][k]\n",
    "            for k in range(K):\n",
    "                responsibilities[i][k] /= total\n",
    "                N[k] += responsibilities[i][k]\n",
    "                \n",
    "        # M-step: Re-estimate the parameters given current responsibilities\n",
    "        for k in range(K):\n",
    "            mu[k] = np.multiply(responsibilities[0][k], data[0])\n",
    "            for i in range(1, n):\n",
    "                mu[k] = np.add(mu[k], np.multiply(responsibilities[i][k], data[i]))\n",
    "            mu[k] = np.multiply(1 / N[k], mu[k])        \n",
    "        for k in range(K):\n",
    "            cov_total = np.full(shape = (d, d), fill_value = 0.0)\n",
    "            for i in range(n):\n",
    "                cov_total += responsibilities[i][k] * np.outer((data[i] - mu[k]), (data[i] - mu[k]))\n",
    "            cov[k] = cov_total / N[k]\n",
    "        for k in range(K):\n",
    "            pi[k] = N[k] / n\n",
    "\n",
    "        new_log_likelihood = sum([math.log(sum([pi[k] * multivariate(data[i], mu[k], cov[k]) for k in range(K)])) for i in range(n)])\n",
    "        \n",
    "        if log_likelihood != None and abs(new_log_likelihood - log_likelihood) < 10 ** -3:\n",
    "            converged = True\n",
    "        \n",
    "        log_likelihood = new_log_likelihood\n",
    "        r = [np.argmax(responsibilities[i]) for i in range(n)]\n",
    "        \n",
    "        number_of_iterations += 1\n",
    "        if number_of_iterations % 5 == 0 and is_random == False:\n",
    "            print(\"Current number of iterations:\", number_of_iterations, \"(max number of iterations is set to 30)\")\n",
    "    \n",
    "    return mu, cov, pi, r, number_of_iterations\n",
    "    \n",
    "mu, cov, pi, r, number_of_iterations = GMM(data, 3)\n",
    "print(\"From a total of\", number_of_iterations, \"iterations, we have the approximation of labeled data sizes as\", [round(size) for size in (pi * len(data))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cba1bcc",
   "metadata": {},
   "source": [
    "2. Implement 2 evaluation metrics including Silhouette Coefficient and Rand Index from scratch (i.e., not calling off-the-shelf package) to evaluate the performance of above clustering algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "28655ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Coefficient for K-means: 0.4859565093552149\n",
      "Silhouette Coefficient for GMM-EM: 0.6705752238105781\n",
      "Rand Index for K-means: 0.638049669628617\n",
      "Rand Index for GMM-EM: 0.8864889496468444\n"
     ]
    }
   ],
   "source": [
    "def silhouette_coefficient(x, c, r, K):\n",
    "    assigned = [[] for k in range(K)]\n",
    "    for i in range(len(r)):\n",
    "        assigned[r[i]].append(i)\n",
    "\n",
    "    silhouette_coefficients = []\n",
    "    for k in range(K):\n",
    "        for i in range(len(assigned[k])):\n",
    "            dist = None\n",
    "            nearest_k = None\n",
    "            for next_k in range(K):\n",
    "                if next_k != k and (dist == None or dist < np.linalg.norm(c[next_k] - x[assigned[k][i]])):\n",
    "                    dist = np.linalg.norm(c[next_k] - x[assigned[k][i]])\n",
    "                    nearest_k = next_k\n",
    "                \n",
    "            intra_cluster_distances = []\n",
    "            nearest_cluster_distances = []\n",
    "            \n",
    "            for j in range(len(assigned[k])):\n",
    "                if i != j: intra_cluster_distances.append(np.linalg.norm(x[assigned[k][i]] - x[assigned[k][j]]))\n",
    "            for j in range(len(assigned[nearest_k])):\n",
    "                nearest_cluster_distances.append(np.linalg.norm(x[assigned[k][i]] - x[assigned[nearest_k][j]]))\n",
    "            \n",
    "            a = sum(intra_cluster_distances) / len(intra_cluster_distances)\n",
    "            b = sum(nearest_cluster_distances) / len(nearest_cluster_distances)\n",
    "            silhouette_coefficients.append((b - a) / max(a, b))\n",
    "        \n",
    "    return sum(silhouette_coefficients) / len(silhouette_coefficients)\n",
    "\n",
    "def rand_index(y, y_expected):\n",
    "    if len(y) != len(y_expected):\n",
    "        return -1\n",
    "    n = len(y)\n",
    "    cnt = 0\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            if (y[i] == y[j] and y_expected[i] == y_expected[j]) or (y[i] != y[j] and y_expected[i] != y_expected[j]):\n",
    "                cnt += 1\n",
    "    return cnt / (n * (n - 1) / 2)\n",
    "\n",
    "f = open(\"seeds_dataset.txt\", \"r\")\n",
    "data = []\n",
    "\n",
    "# data shape should be (210, 8) after read\n",
    "for line in f:\n",
    "    data.append([float(number) for number in line.split('\\t') if number != ''])        \n",
    "\n",
    "x = [sample[:-1] for sample in data]\n",
    "y = [int(sample[-1:][0]) for sample in data]\n",
    "\n",
    "x = np.array(x) \n",
    "print(\"Silhouette Coefficient for K-means:\", silhouette_coefficient(x, ans[2], ans[3], 3))\n",
    "print(\"Silhouette Coefficient for GMM-EM:\", silhouette_coefficient(x, mu, r, 3))\n",
    "print(\"Rand Index for K-means:\", rand_index(ans[3], y))\n",
    "print(\"Rand Index for GMM-EM:\", rand_index(r, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae18b1e",
   "metadata": {},
   "source": [
    "3. Analyze the sensitivity to the initialization of each algorithm (e.g., run one clustering algorithm with random initialization multiple times, and calculate the standard deviations of evaluation scores of these clustering results) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "1acbf360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Deviation of Silhouette Coefficient for K-means: 0.038174147903931574\n",
      "Standard Deviation of Rand Index for K-means: 0.04788233824981838\n",
      "Standard Deviation of Silhouette Coefficient for GMM-EM: 0.16652724813904116\n",
      "Standard Deviation of Rand Index for GMM-EM: 0.10331931286708596\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "f = open(\"seeds_dataset.txt\", \"r\")\n",
    "data = []\n",
    "\n",
    "# data shape should be (210, 8) after read\n",
    "for line in f:\n",
    "    data.append([float(number) for number in line.split('\\t') if number != ''])\n",
    "\n",
    "x = [sample[:-1] for sample in data]\n",
    "y = [int(sample[-1:][0]) for sample in data]\n",
    "data = [sample[:-1] for sample in data]\n",
    "\n",
    "trials = 10\n",
    "\n",
    "silhouette_coefficient_results = []\n",
    "rand_index_results = []\n",
    "for trial in range(trials):\n",
    "    ans = cluster(data, 3, is_random = True)\n",
    "    silhouette_coefficient_results.append(silhouette_coefficient(np.array(x), ans[2], ans[3], 3))\n",
    "    rand_index_results.append(rand_index(ans[3], y))\n",
    "    \n",
    "print(\"Standard Deviation of Silhouette Coefficient for K-means:\", statistics.stdev(silhouette_coefficient_results))\n",
    "print(\"Standard Deviation of Rand Index for K-means:\", statistics.stdev(rand_index_results))\n",
    "\n",
    "\n",
    "trials = 5   \n",
    "\n",
    "silhouette_coefficient_results = []\n",
    "rand_index_results = []\n",
    "for trial in range(trials):\n",
    "    mu, cov, pi, r, number_of_iterations = GMM(data, 3, is_random = True)\n",
    "    silhouette_coefficient_results.append(silhouette_coefficient(np.array(x), mu, r, 3))\n",
    "    rand_index_results.append(rand_index(r, y))    \n",
    "    \n",
    "print(\"Standard Deviation of Silhouette Coefficient for GMM-EM:\", statistics.stdev(silhouette_coefficient_results))\n",
    "print(\"Standard Deviation of Rand Index for GMM-EM:\", statistics.stdev(rand_index_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecc8a16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
