{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center'> CSC4120 Programming Assignment 3  </h1>\n",
    "\n",
    "## Submission Requirements\n",
    "\n",
    "   The submission <font color = #FF0000>deadline is March 17 (Sun.), 2024, 11:59 pm</font>. Solutions submitted after the deadline will be graded as 0 points. Please submit a **ipynb** file using given template and clearly state your group members' student IDs. Otherwise, your points will be deducted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student IDs\n",
    "\n",
    "120040025 - Yohandi\n",
    "\n",
    "120040007 - Andrew Nathanael"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Remote Error Correction\n",
    "\n",
    "Prof. Costas was isolated in Xi'an. He was planning to work on the homework during the isolation. Unfortunately, his computer broke and corrupted the latest copy of the homework file. A few lines of the file were affected. Prof. Costas could just ask Mingjie, who was in Shenzhen, to send him the latest copy of the file. But the hotel he stayed has very slow and pricey Internet access.\n",
    "\n",
    "Help Prof. Costas recover the file and prepare homework on time! Design an interactive protocol for detecting and correcting corrupted lines that uses **little communication**.\n",
    "\n",
    "In this question, you have to answer two short-answer questions and implement the following three functions in file_compare.ipynb.\n",
    "(Feel free to add additional methods)\n",
    "\n",
    "* def hash_function(s)\n",
    "* def compute_node_value(table, i, j)\n",
    "* def binary_check_hash(left, right)\n",
    "\n",
    "First, Prof. Costas and Mingjie need a good hash function that can compute a hash value for any contiguous subset of lines of a file.\n",
    "\n",
    "1. Implement a **division hash** in the function `hash_function`.\n",
    "\n",
    "You can use bulit-in function `hash()` (or any other function) to calculate the hash code of a string, but you should make **your** `hash_function` a division hash. \n",
    "\n",
    "2. Implement the function `compute_node_value`.\n",
    "\n",
    "It computes all useful hash values for binary comparison (to be explained later), and store them in corresponding table (`HashCorruptedFile[][]` and `HashOriginalFile[][]`). It only calculates `HashCorruptedFile[0, n]`, `HashCorruptedFile[0, n/2]`, `HashCorruptedFile[0, n/4]`, `HashCorruptedFile[n/2, n]`, `HashCorruptedFile[n/4, n/2]` ... etc.\n",
    "\n",
    "Hints:\n",
    "- `wordsCorrupted[i]` := data of line i in the corrupted file\n",
    "- `HashCorruptedFile[i][i]` := Hash value of \"line i in the corrupted File\"\n",
    "- `HashCorruptedFile[i][j]` := Hash value of \"lines i to j in the corrupted File\"\n",
    "\n",
    "For example, if the corrupted file (4 lines) is as follows:\n",
    "\n",
    "```raw\n",
    "    aaaa\\n\n",
    "    bbbb\\n\n",
    "    cccc\\n\n",
    "    dddd\n",
    "```\n",
    "\n",
    "then,\n",
    "- `wordsCorrupted[0]` = \"aaaa\"\n",
    "- `HashCorruptedFile[0][0]` = hash function(\"aaaa\")\n",
    "- `HashCorruptedFile[1][1]` = hash function(\"bbbb\")\n",
    "- `HashCorruptedFile[0][2]` = hash function(temp). temp is something related to \"aaaa\",\n",
    "\"bbbb\" and \"cccc\", or related to their hash value. It depends on your own design.\n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Prof. Costas and Mingjie have your hash function. They want to find the corrupted lines. Mingjie came up with a naive algorithm to compare both files line by line (and he wrote the codes in `naive_check_hash`). They could send and compare the hash value of each line. But Prof. Costas is unsatisfied with the communication cost $ \\Theta(n) $. Help them design an efficient algorithm! \n",
    "\n",
    "3. Suppose the two copies of the file differ in some number of consecutive lines.\n",
    "Let $ n $ be the total number of lines in the file. How can Prof. Costas and Mingjie use\n",
    "binary search to find the start and end of the corruption by exchanging only $ O(\\log n) $ hash\n",
    "values? Describe your idea below.\n",
    "\n",
    "The procedure starts with a comparison of the hash value of the entire range.\n",
    "\n",
    "- The hash values of the entire files, both original and corrupted, are compared. If these hash values are identical, it implies the absence of corruption.\n",
    "- If the hash values differ, this indicates the presence of corruption within the file. The file is then divided into two segments: the first half (from start to midpoint) and the second half (from midpoint to end).\n",
    "    - **Finding the start**: To locate the beginning of the corrupted lines, the hash values of the first half of the file are compared between the original and the corrupted versions. A difference in hash values suggests that the corruption starts within this first half. If no difference is found, the search moves to the second half.\n",
    "    - **Finding the end**: To determine the ending of the corrupted lines, a similar comparison is conducted starting with the second half first and the first half afterwards (interchange the order to prioritize the rightmost).\n",
    "\n",
    "The process will be repeated until it finds the smallest part where the original and corrupted file hashes are different. Both start line and end line is computed independently. With that, only $\\mathcal{O}(\\log n)$ of hash values needed to get the information of start and end lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Let $ W $ ($ W \\ll n $) be the number of corrupted lines (not necessarily consecutive). What can\n",
    "they do to find all corrupted lines by exchanging only $ O(W Â· \\log n) $ hash values? Describe your idea below.\n",
    "\n",
    "Assume $W \\geq 1$.\n",
    "\n",
    "- The search begins with the entire file as the initial range $[0, n - 1]$.\n",
    "- A binary search is conducted within the current range to identify the leftmost corrupted line. This involves comparing hash values of segments within the range, starting with a comparison of the first half of the range. If the first half's hash does not match the expected (original) hash, it indicates the presence of corruption within this half, and the search continues within this segment. Conversely, if the first half's hash matches the original, the search moves to the second half, as the corruption must be located there.\n",
    "- After identifying a corrupted line at index $i$, the range is narrowed to $[i+1, end]$ for the next iteration, effectively skipping over the identified corrupted line and focusing the search on remaining lines.\n",
    "- This process is repeated, each time identifying a single corrupted line and then narrowing the search range, until all \\(W\\) corrupted lines are identified. \n",
    "\n",
    "Throughout the process, the binary search prioritizes finding the leftmost corrupted line within the current range. This ensures that only $\\mathcal{O}(W)$ searches are performed, maintaining a complexity of  $\\mathcal{O}(W \\cdot \\log n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Implement `binary_check_hash`.\n",
    "\n",
    "This function simulates the interactive protocol (and is the implementation of your idea in 4). It should call the function `compare` to compare two hash values, print `\"[binary_check_hash] found damaged content at line xx\"` as `naive_check_hash` does whenever a corrupted line is found, and call the function `send_words` to retrieve the original line from the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. starting computation of hash tree...\n",
      "reading files: file_corrupted1.txt file_original1.txt\n",
      "finished computation of hash tree\n",
      "\n",
      "2. starting computation of naive_check_hash()...\n",
      "[naive_check_hash] found damaged content at line 255\n",
      "[naive_check_hash] found damaged content at line 511\n",
      "[naive_check_hash] found damaged content at line 767\n",
      "[naive_check_hash] found damaged content at line 1023\n",
      "cost of navie_check_hash() = 1040 \n",
      "\n",
      "3. starting computation of binary_check_hash()...\n",
      "[binary_check_hash] found damaged content at line 255\n",
      "[binary_check_hash] found damaged content at line 511\n",
      "[binary_check_hash] found damaged content at line 767\n",
      "[binary_check_hash] found damaged content at line 1023\n",
      "cost of binary_check_hash() = 90 \n",
      "\n",
      "1. starting computation of hash tree...\n",
      "reading files: file_corrupted2.txt file_original2.txt\n",
      "finished computation of hash tree\n",
      "\n",
      "2. starting computation of naive_check_hash()...\n",
      "[naive_check_hash] found damaged content at line 255\n",
      "[naive_check_hash] found damaged content at line 511\n",
      "[naive_check_hash] found damaged content at line 767\n",
      "[naive_check_hash] found damaged content at line 1023\n",
      "[naive_check_hash] found damaged content at line 1029\n",
      "[naive_check_hash] found damaged content at line 1036\n",
      "[naive_check_hash] found damaged content at line 1279\n",
      "[naive_check_hash] found damaged content at line 1535\n",
      "[naive_check_hash] found damaged content at line 1791\n",
      "[naive_check_hash] found damaged content at line 2047\n",
      "[naive_check_hash] found damaged content at line 2303\n",
      "[naive_check_hash] found damaged content at line 2559\n",
      "[naive_check_hash] found damaged content at line 2815\n",
      "[naive_check_hash] found damaged content at line 3071\n",
      "[naive_check_hash] found damaged content at line 3327\n",
      "[naive_check_hash] found damaged content at line 3583\n",
      "[naive_check_hash] found damaged content at line 3839\n",
      "[naive_check_hash] found damaged content at line 4095\n",
      "cost of navie_check_hash() = 4168 \n",
      "\n",
      "3. starting computation of binary_check_hash()...\n",
      "[binary_check_hash] found damaged content at line 255\n",
      "[binary_check_hash] found damaged content at line 511\n",
      "[binary_check_hash] found damaged content at line 767\n",
      "[binary_check_hash] found damaged content at line 1023\n",
      "[binary_check_hash] found damaged content at line 1029\n",
      "[binary_check_hash] found damaged content at line 1036\n",
      "[binary_check_hash] found damaged content at line 1279\n",
      "[binary_check_hash] found damaged content at line 1535\n",
      "[binary_check_hash] found damaged content at line 1791\n",
      "[binary_check_hash] found damaged content at line 2047\n",
      "[binary_check_hash] found damaged content at line 2303\n",
      "[binary_check_hash] found damaged content at line 2559\n",
      "[binary_check_hash] found damaged content at line 2815\n",
      "[binary_check_hash] found damaged content at line 3071\n",
      "[binary_check_hash] found damaged content at line 3327\n",
      "[binary_check_hash] found damaged content at line 3583\n",
      "[binary_check_hash] found damaged content at line 3839\n",
      "[binary_check_hash] found damaged content at line 4095\n",
      "cost of binary_check_hash() = 396 \n",
      "\n",
      "1. starting computation of hash tree...\n",
      "reading files: file_corrupted3.txt file_original3.txt\n",
      "finished computation of hash tree\n",
      "\n",
      "2. starting computation of naive_check_hash()...\n",
      "[naive_check_hash] found damaged content at line 255\n",
      "[naive_check_hash] found damaged content at line 511\n",
      "[naive_check_hash] found damaged content at line 767\n",
      "[naive_check_hash] found damaged content at line 1023\n",
      "[naive_check_hash] found damaged content at line 1029\n",
      "[naive_check_hash] found damaged content at line 1036\n",
      "[naive_check_hash] found damaged content at line 1279\n",
      "[naive_check_hash] found damaged content at line 1535\n",
      "[naive_check_hash] found damaged content at line 1791\n",
      "[naive_check_hash] found damaged content at line 2047\n",
      "[naive_check_hash] found damaged content at line 2303\n",
      "[naive_check_hash] found damaged content at line 2559\n",
      "[naive_check_hash] found damaged content at line 2815\n",
      "[naive_check_hash] found damaged content at line 3071\n",
      "[naive_check_hash] found damaged content at line 3327\n",
      "[naive_check_hash] found damaged content at line 3583\n",
      "[naive_check_hash] found damaged content at line 3839\n",
      "[naive_check_hash] found damaged content at line 4095\n",
      "[naive_check_hash] found damaged content at line 4351\n",
      "[naive_check_hash] found damaged content at line 4607\n",
      "[naive_check_hash] found damaged content at line 4863\n",
      "[naive_check_hash] found damaged content at line 5119\n",
      "[naive_check_hash] found damaged content at line 5121\n",
      "[naive_check_hash] found damaged content at line 5140\n",
      "[naive_check_hash] found damaged content at line 5145\n",
      "[naive_check_hash] found damaged content at line 5149\n",
      "[naive_check_hash] found damaged content at line 5206\n",
      "[naive_check_hash] found damaged content at line 5217\n",
      "[naive_check_hash] found damaged content at line 5375\n",
      "[naive_check_hash] found damaged content at line 5631\n",
      "[naive_check_hash] found damaged content at line 5887\n",
      "[naive_check_hash] found damaged content at line 6143\n",
      "[naive_check_hash] found damaged content at line 6399\n",
      "[naive_check_hash] found damaged content at line 6655\n",
      "[naive_check_hash] found damaged content at line 6911\n",
      "[naive_check_hash] found damaged content at line 7167\n",
      "[naive_check_hash] found damaged content at line 7423\n",
      "[naive_check_hash] found damaged content at line 7679\n",
      "[naive_check_hash] found damaged content at line 7935\n",
      "[naive_check_hash] found damaged content at line 8191\n",
      "[naive_check_hash] found damaged content at line 8447\n",
      "[naive_check_hash] found damaged content at line 8703\n",
      "[naive_check_hash] found damaged content at line 8959\n",
      "[naive_check_hash] found damaged content at line 9215\n",
      "[naive_check_hash] found damaged content at line 9471\n",
      "[naive_check_hash] found damaged content at line 9727\n",
      "[naive_check_hash] found damaged content at line 9983\n",
      "[naive_check_hash] found damaged content at line 10239\n",
      "[naive_check_hash] found damaged content at line 10495\n",
      "[naive_check_hash] found damaged content at line 10751\n",
      "[naive_check_hash] found damaged content at line 11007\n",
      "[naive_check_hash] found damaged content at line 11263\n",
      "[naive_check_hash] found damaged content at line 11519\n",
      "[naive_check_hash] found damaged content at line 11775\n",
      "[naive_check_hash] found damaged content at line 12031\n",
      "[naive_check_hash] found damaged content at line 12287\n",
      "[naive_check_hash] found damaged content at line 12543\n",
      "[naive_check_hash] found damaged content at line 12799\n",
      "[naive_check_hash] found damaged content at line 13055\n",
      "[naive_check_hash] found damaged content at line 13311\n",
      "[naive_check_hash] found damaged content at line 13567\n",
      "[naive_check_hash] found damaged content at line 13823\n",
      "[naive_check_hash] found damaged content at line 14079\n",
      "[naive_check_hash] found damaged content at line 14335\n",
      "[naive_check_hash] found damaged content at line 14591\n",
      "[naive_check_hash] found damaged content at line 14847\n",
      "[naive_check_hash] found damaged content at line 15103\n",
      "[naive_check_hash] found damaged content at line 15359\n",
      "[naive_check_hash] found damaged content at line 15615\n",
      "[naive_check_hash] found damaged content at line 15871\n",
      "[naive_check_hash] found damaged content at line 16127\n",
      "[naive_check_hash] found damaged content at line 16383\n",
      "cost of navie_check_hash() = 16672 \n",
      "\n",
      "3. starting computation of binary_check_hash()...\n",
      "[binary_check_hash] found damaged content at line 255\n",
      "[binary_check_hash] found damaged content at line 511\n",
      "[binary_check_hash] found damaged content at line 767\n",
      "[binary_check_hash] found damaged content at line 1023\n",
      "[binary_check_hash] found damaged content at line 1029\n",
      "[binary_check_hash] found damaged content at line 1036\n",
      "[binary_check_hash] found damaged content at line 1279\n",
      "[binary_check_hash] found damaged content at line 1535\n",
      "[binary_check_hash] found damaged content at line 1791\n",
      "[binary_check_hash] found damaged content at line 2047\n",
      "[binary_check_hash] found damaged content at line 2303\n",
      "[binary_check_hash] found damaged content at line 2559\n",
      "[binary_check_hash] found damaged content at line 2815\n",
      "[binary_check_hash] found damaged content at line 3071\n",
      "[binary_check_hash] found damaged content at line 3327\n",
      "[binary_check_hash] found damaged content at line 3583\n",
      "[binary_check_hash] found damaged content at line 3839\n",
      "[binary_check_hash] found damaged content at line 4095\n",
      "[binary_check_hash] found damaged content at line 4351\n",
      "[binary_check_hash] found damaged content at line 4607\n",
      "[binary_check_hash] found damaged content at line 4863\n",
      "[binary_check_hash] found damaged content at line 5119\n",
      "[binary_check_hash] found damaged content at line 5121\n",
      "[binary_check_hash] found damaged content at line 5140\n",
      "[binary_check_hash] found damaged content at line 5145\n",
      "[binary_check_hash] found damaged content at line 5149\n",
      "[binary_check_hash] found damaged content at line 5206\n",
      "[binary_check_hash] found damaged content at line 5217\n",
      "[binary_check_hash] found damaged content at line 5375\n",
      "[binary_check_hash] found damaged content at line 5631\n",
      "[binary_check_hash] found damaged content at line 5887\n",
      "[binary_check_hash] found damaged content at line 6143\n",
      "[binary_check_hash] found damaged content at line 6399\n",
      "[binary_check_hash] found damaged content at line 6655\n",
      "[binary_check_hash] found damaged content at line 6911\n",
      "[binary_check_hash] found damaged content at line 7167\n",
      "[binary_check_hash] found damaged content at line 7423\n",
      "[binary_check_hash] found damaged content at line 7679\n",
      "[binary_check_hash] found damaged content at line 7935\n",
      "[binary_check_hash] found damaged content at line 8191\n",
      "[binary_check_hash] found damaged content at line 8447\n",
      "[binary_check_hash] found damaged content at line 8703\n",
      "[binary_check_hash] found damaged content at line 8959\n",
      "[binary_check_hash] found damaged content at line 9215\n",
      "[binary_check_hash] found damaged content at line 9471\n",
      "[binary_check_hash] found damaged content at line 9727\n",
      "[binary_check_hash] found damaged content at line 9983\n",
      "[binary_check_hash] found damaged content at line 10239\n",
      "[binary_check_hash] found damaged content at line 10495\n",
      "[binary_check_hash] found damaged content at line 10751\n",
      "[binary_check_hash] found damaged content at line 11007\n",
      "[binary_check_hash] found damaged content at line 11263\n",
      "[binary_check_hash] found damaged content at line 11519\n",
      "[binary_check_hash] found damaged content at line 11775\n",
      "[binary_check_hash] found damaged content at line 12031\n",
      "[binary_check_hash] found damaged content at line 12287\n",
      "[binary_check_hash] found damaged content at line 12543\n",
      "[binary_check_hash] found damaged content at line 12799\n",
      "[binary_check_hash] found damaged content at line 13055\n",
      "[binary_check_hash] found damaged content at line 13311\n",
      "[binary_check_hash] found damaged content at line 13567\n",
      "[binary_check_hash] found damaged content at line 13823\n",
      "[binary_check_hash] found damaged content at line 14079\n",
      "[binary_check_hash] found damaged content at line 14335\n",
      "[binary_check_hash] found damaged content at line 14591\n",
      "[binary_check_hash] found damaged content at line 14847\n",
      "[binary_check_hash] found damaged content at line 15103\n",
      "[binary_check_hash] found damaged content at line 15359\n",
      "[binary_check_hash] found damaged content at line 15615\n",
      "[binary_check_hash] found damaged content at line 15871\n",
      "[binary_check_hash] found damaged content at line 16127\n",
      "[binary_check_hash] found damaged content at line 16383\n",
      "cost of binary_check_hash() = 1584 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hash Values for words in New File\n",
    "# HashCorruptedFile[i,j] = Hash value of lines i to j in Corrupted File\n",
    "HashCorruptedFile = []\n",
    "\n",
    "# Hash Values for words in Old File\n",
    "# HashOriginalFile[i,j] = Hash value of lines i to j in Original File\n",
    "HashOriginalFile = []\n",
    "\n",
    "# Words in the Corrupted and Original File\n",
    "wordsCorrupted = []\n",
    "wordsOriginal = []\n",
    "\n",
    "# n : number of lines in each file\n",
    "n = 1 # default value\n",
    "\n",
    "# variable to measure network transmission cost\n",
    "cost = 0\n",
    "\n",
    "# penalty value for transmitting the whole line\n",
    "penalty = 4\n",
    "\n",
    "\n",
    "# feel free to add additional functions\n",
    "\n",
    "def hash_function(s):\n",
    "    '''\n",
    "    compute and return the hash value of the string s.\n",
    "\n",
    "    You can use built-in hash() (or any other method) to calculate a hash code. But you should finally return a division hash code (something % something). \n",
    "    '''\n",
    "    divisor = 10**9 + 7\n",
    "    return hash(s) % divisor\n",
    "\n",
    "def compute_node_values():\n",
    "    ''' \n",
    "    in the first part of compute_hash_tree(), we computed HashOriginalFile[0][0], HashOriginalFile[1][1], HashOriginalFile[2][2].... etc\n",
    "    \n",
    "    Now, here in compute_node_values(), you have to calculate HashOriginalFile[i][j] where i < j\n",
    "    So, HashOriginalFile[2][4] = hash_function(\"line 2-4 of corrupted file\"). It depends on your own design. \n",
    "    \n",
    "    For example, HashOriginalFile[2][4] can be = hash_function(hash_function(line 2) + 2*hash_function(line 3) + 3*hash_function(line 4)).... but it is good? Perhaps not. So, write your own hash function here! \n",
    "    '''\n",
    "    compute_node_value(HashCorruptedFile, wordsCorrupted, 0, n - 1)\n",
    "    compute_node_value(HashOriginalFile, wordsOriginal, 0, n - 1)\n",
    "\n",
    "def compute_node_value(table, words, i, j):\n",
    "    '''\n",
    "    it calculates table[i][j], where table could be HashCorruptedFile or HashOriginalFile.\n",
    "\n",
    "    Remember you are doing a binary search here. It's not necessary to try out all combination of i, j. You only need to consider (0, n), (0, n/2), (n/2, n), ... \n",
    "    '''\n",
    "    if i == j:\n",
    "        table[i][j] = hash_function(words[i])\n",
    "    else:\n",
    "        mid = (i + j) // 2\n",
    "\n",
    "        if table[i][mid] is None:\n",
    "            table[i][mid] = compute_node_value(table, words, i, mid)\n",
    "        if table[mid + 1][j] is None:\n",
    "            table[mid + 1][j] = compute_node_value(table, words, mid + 1, j)\n",
    "\n",
    "        table[i][j] = hash_function(str(table[i][mid]) + str(table[mid + 1][j]))\n",
    "\n",
    "        return table[i][j]\n",
    "\n",
    "\n",
    "def binary_check_hash(left, right):\n",
    "    ''' \n",
    "    performs binary search over the hash tree.\n",
    "\n",
    "    Search through the whole tree. \n",
    "    The output should be similar to naive_check_hash().\n",
    "    '''\n",
    "    global cost, penalty\n",
    "    if left > right or left < 0 or right >= n:\n",
    "        return\n",
    "    \n",
    "    if left == right:\n",
    "        if not compare(HashCorruptedFile[left][left], HashOriginalFile[left][left]):\n",
    "            print(\"[binary_check_hash] found damaged content at line\", left)\n",
    "            send_word()\n",
    "        return\n",
    "    \n",
    "    mid = (left + right) // 2\n",
    "\n",
    "    if not compare(hash_function(\"\".join(wordsCorrupted[left:mid+1])), hash_function(\"\".join(wordsOriginal[left:mid+1]))):\n",
    "        binary_check_hash(left, mid)\n",
    "    \n",
    "    if not compare(hash_function(\"\".join(wordsCorrupted[mid+1:right+1])), hash_function(\"\".join(wordsOriginal[mid+1:right+1]))):\n",
    "        binary_check_hash(mid + 1, right)\n",
    "\n",
    "# read fileCorrupted and fileOriginal, save to wordsCorrupted and wordsOriginal\n",
    "def read_files(fileCorrupted, fileOriginal):\n",
    "    print(\"reading files:\", fileCorrupted, fileOriginal)\n",
    "    with open(fileCorrupted, 'r') as fCorrupted:\n",
    "        for line in fCorrupted:\n",
    "            wordsCorrupted.append(line.rstrip())\n",
    "    with open(fileOriginal, 'r') as fOriginal:\n",
    "        for line in fOriginal:\n",
    "            wordsOriginal.append(line.rstrip())\n",
    "\n",
    "\n",
    "# Compute the hash values of the leaf nodes in the Hash Tree\n",
    "def compute_hash_tree():\n",
    "    for i in range(n):\n",
    "        HashCorruptedFile.append([])\n",
    "        HashOriginalFile.append([])\n",
    "        for _ in range(n):\n",
    "            HashCorruptedFile[i].append(None)\n",
    "            HashOriginalFile[i].append(None)\n",
    "\n",
    "    for i in range(n):\n",
    "        HashCorruptedFile[i][i] = hash_function(wordsCorrupted[i])\n",
    "        HashOriginalFile[i][i] = hash_function(wordsOriginal[i])\n",
    "\n",
    "    compute_node_values()\n",
    "\n",
    "\n",
    "# compares the two hash values. total cost increase by 1\n",
    "def compare(oldhash, newhash):\n",
    "    global cost\n",
    "    cost += 1\n",
    "    return oldhash == newhash\n",
    "\n",
    "\n",
    "# adds the penalty of sending the whole line over network transmission\n",
    "def send_word():\n",
    "    global cost, penalty\n",
    "    cost += penalty\n",
    "\n",
    "# naive hash check method, goes over all the lines and compares them.\n",
    "def naive_check_hash(left, right):\n",
    "    global cost, penalty\n",
    "    for i in range(left, right + 1):\n",
    "        if not compare(HashCorruptedFile[i][i], HashOriginalFile[i][i]):\n",
    "            print(\"[naive_check_hash] found damaged content at line\", i)\n",
    "            send_word()\n",
    "\n",
    "\n",
    "# performs the naive_check_hash algorithm and binary_check_hash algorithm.\n",
    "# compares filename1 and filename2 with n lines each\n",
    "# print out the cost\n",
    "def file_transfer(filename1, filename2, n1):\n",
    "    global cost, n\n",
    "    n = n1\n",
    "\n",
    "    print(\"1. starting computation of hash tree...\")\n",
    "    read_files(filename1, filename2)\n",
    "    compute_hash_tree()\n",
    "    print(\"finished computation of hash tree\\n\")\n",
    "\n",
    "    print(\"2. starting computation of naive_check_hash()...\")\n",
    "    cost = 0\n",
    "    naive_check_hash(0, n - 1)\n",
    "    print(\"cost of navie_check_hash() =\", cost, \"\\n\")\n",
    "\n",
    "    print(\"3. starting computation of binary_check_hash()...\")\n",
    "    cost = 0\n",
    "    binary_check_hash(0, n - 1)\n",
    "    print(\"cost of binary_check_hash() =\", cost, \"\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_transfer(\"file_corrupted1.txt\", \"file_original1.txt\", 1024)\n",
    "    file_transfer(\"file_corrupted2.txt\", \"file_original2.txt\", 4096)\n",
    "    file_transfer(\"file_corrupted3.txt\", \"file_original3.txt\", 16384)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "90244c378c7e3257c2f5a89cc828b05c1d67956a5c0682215f78ff63c5f4d2c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
